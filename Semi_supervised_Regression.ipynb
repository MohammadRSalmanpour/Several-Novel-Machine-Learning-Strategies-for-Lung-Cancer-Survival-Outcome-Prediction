{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT8iRw1QjtiH"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import (GridSearchCV, cross_validate, train_test_split)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import (AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor,\n",
        "                              RandomForestRegressor)\n",
        "from sklearn.feature_selection import (f_regression, mutual_info_regression,\n",
        "                                       r_regression)\n",
        "from sklearn.linear_model import Lasso, LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwR82TthAZaQ"
      },
      "outputs": [],
      "source": [
        "n_feats = [5,10,50]\n",
        "fss = [r_regression,f_regression,mutual_info_regression]\n",
        "\n",
        "result_path = \"/\"\n",
        "feature_path = \"/\"\n",
        "\n",
        "def append_row(df, row):\n",
        "    return pd.concat([\n",
        "                df,\n",
        "                pd.DataFrame([row], columns=row.index)]\n",
        "          ).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SybtX39ns5ll"
      },
      "outputs": [],
      "source": [
        "#@title Regressors & Params\n",
        "\n",
        "# Define the parameter spaces for each regressor\n",
        "knn_param_space = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "}\n",
        "\n",
        "abr_param_space = {\n",
        "    'n_estimators': [10],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'loss': ['linear', 'square']\n",
        "}\n",
        "\n",
        "lr_param_space = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'positive': [True, False]\n",
        "}\n",
        "\n",
        "svr_param_space = {\n",
        "    'kernel': ['linear', 'poly', 'rbf'],\n",
        "    'C': [0.01, 1],\n",
        "    'degree': [2, 3, 4]\n",
        "}\n",
        "\n",
        "dtr_param_space = {\n",
        "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "rfr_param_space = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "mlpr_param_space = {\n",
        "    'hidden_layer_sizes': [(50,), (50, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'learning_rate': ['constant', 'adaptive']\n",
        "}\n",
        "br_param_space = {\n",
        "    'estimator': [ DecisionTreeRegressor()],\n",
        "    'n_estimators': [10]\n",
        "}\n",
        "\n",
        "gb_param_space = {\n",
        "    'n_estimators': [10],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "}\n",
        "\n",
        "ls_param_space = {\n",
        "    'alpha': [0.01, 1, 10],\n",
        "    'fit_intercept': [True, False],\n",
        "    'positive': [True, False],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "# Define the regressors and their respective parameter spaces\n",
        "regressors = {\n",
        "    'GradientBoostingRegressor':(GradientBoostingRegressor(),gb_param_space),\n",
        "    'BaggingRegressor' :(BaggingRegressor(),br_param_space),\n",
        "    'KNeighborsRegressor': (KNeighborsRegressor(), knn_param_space),\n",
        "    'AdaBoostRegressor': (AdaBoostRegressor(), abr_param_space),\n",
        "    'LinearRegression': (LinearRegression(), lr_param_space),\n",
        "    'SVR': (SVR(), svr_param_space),\n",
        "    'DecisionTreeRegressor': (DecisionTreeRegressor(), dtr_param_space),\n",
        "    'RandomForestRegressor': (RandomForestRegressor(), rfr_param_space),\n",
        "    'MLPRegressor': (MLPRegressor(), mlpr_param_space),\n",
        "    'Lasso':(Lasso(),ls_param_space)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpwRZaipT4oU"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "files = glob.glob(feature_path +\"*\")\n",
        "datasets = []\n",
        "for filee in files:\n",
        "  datasets.append(filee.split(\"/\")[-1].split(\".\")[0])\n",
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nigulunCe_hx"
      },
      "outputs": [],
      "source": [
        "#@title Semi-Supervised\n",
        "runs_df = pd.DataFrame()\n",
        "\n",
        "n = 0\n",
        "# Step 1: Prepare your data\n",
        "for dataset in datasets:\n",
        "  for main_name, (main_reg, main_param_space) in regressors.items():\n",
        "    n=n+1\n",
        "    path = feature_path + dataset + \".xlsx\"\n",
        "    Y = pd.read_excel(path, sheet_name='Output' , engine='openpyxl',header=0)\n",
        "    X = pd.read_excel(path, sheet_name='Data' , engine='openpyxl',header=0).reindex()\n",
        "\n",
        "    YLabeled = pd.DataFrame(Y[Y['Duration']!=0]['Duration'])/365\n",
        "    YLabeldindxs = Y[Y['Duration']!=0].index\n",
        "    XLabeled = X.loc[YLabeldindxs]\n",
        "    YLabeled = YLabeled['Duration']\n",
        "\n",
        "    YUnlabeld = pd.DataFrame(Y[Y['Duration']==0]['Duration'])/365\n",
        "    YUnlabeldindxs = Y[Y['Duration']==0].index\n",
        "    XUnlabeld = X.loc[YUnlabeldindxs]\n",
        "    YUnlabeld = YUnlabeld['Duration']\n",
        "\n",
        "    XLabeled.columns = XLabeled.columns.astype(str)\n",
        "    XUnlabeld.columns = XUnlabeld.columns.astype(str)\n",
        "\n",
        "    # Step 2: Split the data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(XLabeled, YLabeled, test_size=0.2,random_state=101)\n",
        "\n",
        "    # Step 3: Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    XUnlabeld = scaler.transform(XUnlabeld)\n",
        "\n",
        "    #  Create PCA instance: PCA for 5 components\n",
        "    pca = PCA(n_components=5)\n",
        "    X_train = pca.fit_transform(X_train)\n",
        "    X_test = pca.transform(X_test)\n",
        "    XUnlabeld = pca.transform(XUnlabeld)\n",
        "\n",
        "    # Step 4: Grid search\n",
        "    pseudo_grid_search = GridSearchCV(estimator=MLPRegressor(), param_grid=mlpr_param_space, cv=5)\n",
        "    pseudo_grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Step 5: Retrieve the best model from grid search\n",
        "    pseudo_best_regressor = pseudo_grid_search.best_estimator_\n",
        "\n",
        "    # Step 6: Predict pseudo-labels\n",
        "    pseudo_labels = pseudo_best_regressor.predict(XUnlabeld)\n",
        "\n",
        "    # Step 7: Combine the labeled and pseudo-labeled data\n",
        "    X_combined = np.concatenate((X_train, XUnlabeld), axis=0)\n",
        "    y_combined = np.concatenate((y_train, pseudo_labels), axis=0)\n",
        "\n",
        "    main_grid_search = GridSearchCV(estimator=main_reg, param_grid=main_param_space, cv=5)\n",
        "    main_grid_search.fit(X_combined, y_combined)\n",
        "\n",
        "    main_best_regressor = main_grid_search.best_estimator_\n",
        "\n",
        "    # # Step 8: Train the final model on the combined dataset\n",
        "    main_best_regressor.fit(X_combined, y_combined)\n",
        "\n",
        "    # Step 9: Evaluate the best model using cross-validation\n",
        "    scores = cross_validate(main_best_regressor, X_combined, y_combined, cv=5, scoring='neg_mean_absolute_error')\n",
        "    mae_cv = -np.mean(scores['test_score'])\n",
        "    std_cv = np.std(scores['test_score'])\n",
        "\n",
        "    # Step 3: Make predictions on the test set\n",
        "    y_pred = main_best_regressor.predict(X_test)\n",
        "\n",
        "    # Step 4: Calculate the mean absolute error\n",
        "    mae_tst = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    fs_str = str(fs).split(\" \")[1]\n",
        "\n",
        "    print(str(n*100/170)+','+main_name+','+dataset+\",\"+str(mae_cv)+\",\"+str(std_cv)+\",\"+str(mae_tst))\n",
        "\n",
        "    run_new_row = pd.Series({\"pseudo_ALG\" : \"MLP\" ,\"main_ALG\" : main_name ,\"DATASET\" : dataset\n",
        "                    ,\"method\":\"SSL\"\n",
        "                    ,\"vMAE\":str(mae_cv)\n",
        "                    ,\"vSTD\":str(std_cv)\n",
        "                    ,\"eMAE\":str(mae_tst)\n",
        "                    ,\"vs\":(scores['test_score'])})\n",
        "    runs_df = append_row(runs_df, run_new_row)\n",
        "\n",
        "runs_df.to_csv(result_path+\"/results.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
